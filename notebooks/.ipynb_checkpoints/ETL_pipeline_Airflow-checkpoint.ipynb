{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae6e5c4",
   "metadata": {},
   "source": [
    "# Задача"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67287c68",
   "metadata": {},
   "source": [
    "Создать ETL пайплайн в Airflow для ежедневного расчета нужной таблицы и добавления ее в Clickhouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e0bf58",
   "metadata": {},
   "source": [
    "# Код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ea75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.decorators import dag, task\n",
    "from datetime import datetime, timedelta\n",
    "import pandahouse\n",
    "import pandas as pd\n",
    "\n",
    "# Определение параметров по умолчанию для DAG\n",
    "default_args = {\n",
    "    'owner': 's.krupnov',\n",
    "    'depends_on_past': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'start_date': datetime(2023, 3, 24)\n",
    "}\n",
    "\n",
    "# Параметры подключения к ClickHouse для загрузки данных\n",
    "connection_load = {\n",
    "    'host': 'https://clickhouse.lab.karpov.courses',\n",
    "    'database': 'test',\n",
    "    'user': 'student-rw',\n",
    "    'password': '656e2b0c9c'\n",
    "}\n",
    "\n",
    "# Параметры подключения к ClickHouse для чтения данных\n",
    "connection = {\n",
    "    'host': 'https://clickhouse.lab.karpov.courses/',\n",
    "    'password': 'dpo_python_2020',\n",
    "    'user': 'student',\n",
    "    'database': 'simulator_20230220'\n",
    "}\n",
    "\n",
    "# SQL-запросы для извлечения данных из ClickHouse\n",
    "q_feed = '''\n",
    "SELECT toDate(time) as event_date, user_id,\n",
    "sum(action = 'like') as likes,\n",
    "sum(action = 'view') as views\n",
    "FROM simulator_20230220.feed_actions\n",
    "WHERE toDate(time) = today() - 1\n",
    "GROUP BY event_date, user_id order by user_id\n",
    "'''\n",
    "\n",
    "q_message = '''\n",
    "    SELECT DISTINCT user_id,\n",
    "                    event_date,\n",
    "                    messages_sent,\n",
    "                    users_sent,\n",
    "                    messages_received,\n",
    "                    users_received\n",
    "    FROM (\n",
    "        SELECT toDate(time) as event_date,\n",
    "               user_id,\n",
    "               count(receiver_id) as messages_sent,\n",
    "               count(DISTINCT receiver_id) as users_sent\n",
    "        FROM simulator_20230220.message_actions\n",
    "        WHERE toDate(time) = today\n",
    "        GROUP BY event_date, user_id\n",
    "        ORDER BY user_id\n",
    "    ) t1\n",
    "    LEFT OUTER JOIN (\n",
    "        SELECT toDate(time) as event_date,\n",
    "               receiver_id as user_id,\n",
    "               count(user_id) as messages_received,\n",
    "               count(DISTINCT user_id) as users_received\n",
    "        FROM simulator_20230220.message_actions\n",
    "        WHERE toDate(time) = today()\n",
    "        GROUP BY event_date, user_id\n",
    "        ORDER BY user_id\n",
    "    ) t2\n",
    "    ON t1.user_id = t2.user_id\n",
    "    ORDER BY user_id\n",
    "'''\n",
    "\n",
    "q_slices = '''\n",
    "    SELECT DISTINCT user_id, os, gender, age\n",
    "    FROM (\n",
    "        SELECT DISTINCT user_id, os, gender, age\n",
    "        FROM simulator_20230220.feed_actions\n",
    "        WHERE toDate(time) = today() - 1\n",
    "        ORDER BY user_id\n",
    "        UNION ALL\n",
    "        SELECT DISTINCT user_id, os, gender, age\n",
    "        FROM simulator_20230220.message_actions\n",
    "        WHERE toDate(time) = today()\n",
    "        ORDER BY user_id\n",
    "    )\n",
    "    ORDER BY user_id\n",
    "'''\n",
    "\n",
    "# SQL-запрос для создания таблицы в ClickHouse\n",
    "query_up_load = '''\n",
    "CREATE TABLE IF NOT EXISTS test.etl_table_s_krupnov (\n",
    "    event_date Date,\n",
    "    dimension String,\n",
    "    dimension_value String,\n",
    "    likes UInt64,\n",
    "    views UInt64,\n",
    "    messages_sent UInt64,\n",
    "    users_sent UInt64,\n",
    "    messages_received UInt64,\n",
    "    users_received UInt64\n",
    ") ENGINE = MergeTree()\n",
    "ORDER BY event_date\n",
    "'''\n",
    "\n",
    "# Определение DAG с параметрами\n",
    "@dag(default_args=default_args, schedule_interval='0 11 * * *', catchup=False)\n",
    "def dag_s_krupnov():\n",
    "    \"\"\"\n",
    "    Этот DAG выполняет процесс ETL для данных из ClickHouse, выполняя следующие шаги:\n",
    "    1. Извлечение данных из ClickHouse.\n",
    "    2. Обработка и объединение данных.\n",
    "    3. Группировка данных по различным срезам.\n",
    "    4. Создание окончательной таблицы.\n",
    "    5. Загрузка данных в ClickHouse.\n",
    "    \"\"\"\n",
    "\n",
    "    # Задача для извлечения данных из ClickHouse\n",
    "    @task\n",
    "    def extract(query: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Извлекает данные из ClickHouse с использованием предоставленного SQL-запроса.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): SQL-запрос для извлечения данных.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Извлеченные данные в форме DataFrame.\n",
    "        \"\"\"\n",
    "        df = pandahouse.read_clickhouse(query, connection=connection)\n",
    "        return df\n",
    "\n",
    "    # Задача для извлечения срезов данных из ClickHouse\n",
    "    @task\n",
    "    def extract_slices(query: str, slice_col: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Извлекает срезы данных из ClickHouse с использованием предоставленного SQL-запроса.\n",
    "\n",
    "        Parameters:\n",
    "        - query (str): SQL-запрос для извлечения данных.\n",
    "        - slice_col (str): Имя столбца, по которому нужно выполнить срез.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Извлеченные срезы данных в форме DataFrame.\n",
    "        \"\"\"\n",
    "        df = pandahouse.read_clickhouse(query, connection=connection)\n",
    "        df = df[['user_id', slice_col]].copy()\n",
    "        return df\n",
    "\n",
    "    # Задача для объединения двух DataFrame\n",
    "    @task\n",
    "    def join_tables(t1: pd.DataFrame, t2: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Объединяет два DataFrame по столбцу 'user_id'.\n",
    "\n",
    "        Parameters:\n",
    "        - t1 (pd.DataFrame): Первый DataFrame.\n",
    "        - t2 (pd.DataFrame): Второй DataFrame.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Объединенный DataFrame.\n",
    "        \"\"\"\n",
    "        df_merge = pd.merge(left=t1, right=t2, how='outer', on=['user_id'])\n",
    "        df_merge = (\n",
    "            df_merge.sort_values(by=['user_id'])\n",
    "            .rename({'event_date_x': 'event_date'}, axis=1)\n",
    "            .reset_index()\n",
    "            .drop(columns=['index', 'event_date_y'])\n",
    "        )\n",
    "        df_merge['event_date'] = df_merge['event_date'].fillna(\n",
    "            df_merge['event_date'].mode()[0]\n",
    "        )\n",
    "        df_merge.fillna(0, inplace=True)\n",
    "        df_merge = df_merge.astype({\n",
    "            'likes': 'int',\n",
    "            'views': 'int',\n",
    "            'messages_sent': 'int',\n",
    "            'users_sent': 'int',\n",
    "            'messages_received': 'int',\n",
    "            'users_received': 'int'\n",
    "        })\n",
    "        return df_merge\n",
    "\n",
    "    # Задача для группировки данных по срезу\n",
    "    @task\n",
    "    def get_group_slice(df_merge, slice_, column):\n",
    "        \"\"\"\n",
    "        Группирует данные по указанному срезу.\n",
    "\n",
    "        Parameters:\n",
    "        - df_merge (pd.DataFrame): DataFrame для группировки.\n",
    "        - slice_ (pd.DataFrame): DataFrame с данными среза.\n",
    "        - column (str): Имя столбца для группировки.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Группированный DataFrame.\n",
    "        \"\"\"\n",
    "        merge_slice_ = pd.concat([df_merge, slice_], join='outer', axis=1).fillna(0).drop(columns='user_id')\n",
    "        gr_gender = (\n",
    "            merge_slice_.groupby(['event_date', column])\n",
    "            .agg({\n",
    "                'likes': 'sum',\n",
    "                'views': 'sum',\n",
    "                'messages_sent': 'sum',\n",
    "                'users_sent': 'sum',\n",
    "                'messages_received': 'sum',\n",
    "                'users_received': 'sum'\n",
    "            })\n",
    "            .reset_index()\n",
    "            .rename({column: 'dimension_value'}, axis=1)\n",
    "        )\n",
    "        gr_gender.insert(1, \"dimension\", column)\n",
    "        return gr_gender\n",
    "\n",
    "    # Задача для создания окончательной таблицы\n",
    "    @task\n",
    "    def create_final_table(dfs: list) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Создает окончательную таблицу путем объединения списка DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - dfs (list): Список DataFrame для объединения.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: Окончательная таблица.\n",
    "        \"\"\"\n",
    "        df_final = pd.concat(dfs, join='outer', axis=0)\n",
    "        return df_final\n",
    "\n",
    "    # Задача для загрузки данных в ClickHouse\n",
    "    @task\n",
    "    def load(df):\n",
    "        \"\"\"\n",
    "        Загружает данные в ClickHouse.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pd.DataFrame): DataFrame для загрузки.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        df_feed = extract(q_feed)\n",
    "        df_message = extract(q_message)\n",
    "        slice_os = extract_slices(a_slices, 'os')\n",
    "        slice_gender = extract_slices(q_slices, 'gender')\n",
    "        slice_age = extract_slices(q_slices, 'age')\n",
    "        df_merge = join_tables(df_feed, df_message)\n",
    "        df_gr_os = get_group_slice(df_merge, slice_os, 'os')\n",
    "        df_gr_age = get_group_slice(df_merge, slice_age, 'age')\n",
    "        df_gr_gender = get_group_slice(df_merge, slice_gender, 'gender')\n",
    "        dfs = [df_gr_os, df_gr_age, df_gr_gender]\n",
    "        final_table = create_final_table(dfs)\n",
    "        load(final_table)\n",
    "\n",
    "dag_s_krupnov = dag_s_krupnov()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
